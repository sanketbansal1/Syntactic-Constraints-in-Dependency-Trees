# -*- coding: utf-8 -*-
"""CGS Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1yHRxauB-SZmJk1AS8VHi0c5bteFyRT
"""

pip install conllu networkx

from google.colab import files
uploaded = files.upload()

import io
import networkx as nx
from conllu import parse_incr

def load_sentences_from_conllu_from_colab(train):
    filename = next(iter(train))
    file_content = io.StringIO(train[filename].decode("utf-8"))
    for tokenlist in parse_incr(file_content):
        yield tokenlist

def sentence_to_graph(tokenlist):
    G = nx.DiGraph()
    G.add_node(0, form="ROOT", upos="ROOT", head=None)
    for token in tokenlist:
        if isinstance(token["id"], int):
            idx = token["id"]
            head = token["head"]
            upos = token["upos"]
            G.add_node(idx, form=token["form"], upos=upos, head=head)
            G.add_edge(head, idx)
    return G

def count_intervening_verbal_heads(G):
    intervening_counts = []
    for head, dep in G.edges():
        if head == 0 or dep == 0:
            continue
        min_idx, max_idx = min(head, dep), max(head, dep)
        count = 0
        for node in G.nodes():
            if node == 0:
                continue
            if min_idx < node < max_idx and G.nodes[node]['upos'] == 'VERB':
                count += 1
        intervening_counts.append({
            "head": head,
            "dep": dep,
            "head_form": G.nodes[head]['form'],
            "dep_form": G.nodes[dep]['form'],
            "intervening_verbal_heads": count
        })
    return intervening_counts

all_counts = []
for tokenlist in load_sentences_from_conllu_from_colab(uploaded):
    G = sentence_to_graph(tokenlist)
    counts = count_intervening_verbal_heads(G)
    all_counts.extend(counts)

for item in all_counts:
    print(f"Head: {item['head_form']} -> Dep: {item['dep_form']}, Intervening VERB heads: {item['intervening_verbal_heads']}")